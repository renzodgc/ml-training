{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ef15155-cbf7-4166-9ef0-e011afc8b990",
   "metadata": {},
   "source": [
    "# Section 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e42ebc6-12fa-4ec7-913a-68c98b67b1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql import functions\n",
    "from pyspark.sql import functions as func\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, LongType\n",
    "import collections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79ea0a3a-e6cd-498c-88f5-9cc2ce5739bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SparkSession (Note, the config section is only for Windows!)\n",
    "spark = SparkSession.builder.appName(\"SparkSQL\").master(\"local[*]\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26984ca6-5937-4377-9013-ab3a46689a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = \"../spark_course_resources\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8296144-0521-46e0-a636-11525dbf7776",
   "metadata": {},
   "source": [
    "## Popular Movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db51adaf-5e7b-42e7-8b55-10bbbb3ec2c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|movieID|count|\n",
      "+-------+-----+\n",
      "|     50|  583|\n",
      "|    258|  509|\n",
      "|    100|  508|\n",
      "|    181|  507|\n",
      "|    294|  485|\n",
      "|    286|  481|\n",
      "|    288|  478|\n",
      "|      1|  452|\n",
      "|    300|  431|\n",
      "|    121|  429|\n",
      "|    174|  420|\n",
      "|    127|  413|\n",
      "|     56|  394|\n",
      "|      7|  392|\n",
      "|     98|  390|\n",
      "|    237|  384|\n",
      "|    117|  378|\n",
      "|    172|  367|\n",
      "|    222|  365|\n",
      "|    313|  350|\n",
      "+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "\n",
      "\n",
      "Star Wars (1977): 583\n",
      "Contact (1997): 509\n",
      "Fargo (1996): 508\n",
      "Return of the Jedi (1983): 507\n",
      "Liar Liar (1997): 485\n",
      "English Patient, The (1996): 481\n",
      "Scream (1996): 478\n",
      "Toy Story (1995): 452\n",
      "Air Force One (1997): 431\n",
      "Independence Day (ID4) (1996): 429\n"
     ]
    }
   ],
   "source": [
    "def loadMovieNames():\n",
    "    movieNames = {}\n",
    "    with open(f\"{DATA_ROOT}/ml-100k/u.item\", \"r\", encoding='ISO-8859-1', errors='ignore') as f:\n",
    "        for line in f:\n",
    "            fields = line.split('|')\n",
    "            movieNames[int(fields[0])] = fields[1]\n",
    "    return movieNames\n",
    "\n",
    "# Load up our movie ID -> name dictionary\n",
    "nameDict = loadMovieNames()\n",
    "\n",
    "# Get the raw data\n",
    "lines = spark.sparkContext.textFile(f\"{DATA_ROOT}/ml-100k/u.data\")\n",
    "# Convert it to a RDD of Row objects\n",
    "movies = lines.map(lambda x: Row(movieID =int(x.split()[1])))\n",
    "# Convert that to a DataFrame\n",
    "movieDataset = spark.createDataFrame(movies)\n",
    "\n",
    "# Some SQL-style magic to sort all movies by popularity in one line!\n",
    "topMovieIDs = movieDataset.groupBy(\"movieID\").count().orderBy(\"count\", ascending=False).cache()\n",
    "\n",
    "# Show the results at this point:\n",
    "\n",
    "#|movieID|count|\n",
    "#+-------+-----+\n",
    "#|     50|  584|\n",
    "#|    258|  509|\n",
    "#|    100|  508|\n",
    "\n",
    "topMovieIDs.show()\n",
    "\n",
    "# Grab the top 10\n",
    "top10 = topMovieIDs.take(10)\n",
    "\n",
    "# Print the results\n",
    "print(\"\\n\")\n",
    "for result in top10:\n",
    "    # Each row has movieID, count as above.\n",
    "    print(\"%s: %d\" % (nameDict[result[0]], result[1]))\n",
    "\n",
    "# Stop the session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a829a620-9a4e-4dab-806a-e65ce8d6dfca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-----------------------------+\n",
      "|movieID|count|movieTitle                   |\n",
      "+-------+-----+-----------------------------+\n",
      "|50     |583  |Star Wars (1977)             |\n",
      "|258    |509  |Contact (1997)               |\n",
      "|100    |508  |Fargo (1996)                 |\n",
      "|181    |507  |Return of the Jedi (1983)    |\n",
      "|294    |485  |Liar Liar (1997)             |\n",
      "|286    |481  |English Patient, The (1996)  |\n",
      "|288    |478  |Scream (1996)                |\n",
      "|1      |452  |Toy Story (1995)             |\n",
      "|300    |431  |Air Force One (1997)         |\n",
      "|121    |429  |Independence Day (ID4) (1996)|\n",
      "+-------+-----+-----------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nameDict = spark.sparkContext.broadcast(loadMovieNames())\n",
    "\n",
    "# Create schema when reading u.data\n",
    "schema = StructType([ \\\n",
    "                     StructField(\"userID\", IntegerType(), True), \\\n",
    "                     StructField(\"movieID\", IntegerType(), True), \\\n",
    "                     StructField(\"rating\", IntegerType(), True), \\\n",
    "                     StructField(\"timestamp\", LongType(), True)])\n",
    "\n",
    "# Load up movie data as dataframe\n",
    "moviesDF = spark.read.option(\"sep\", \"\\t\").schema(schema).csv(f\"{DATA_ROOT}/ml-100k/u.data\")\n",
    "\n",
    "movieCounts = moviesDF.groupBy(\"movieID\").count()\n",
    "\n",
    "# Create a user-defined function to look up movie names from our broadcasted dictionary\n",
    "def lookupName(movieID):\n",
    "    return nameDict.value[movieID]\n",
    "\n",
    "lookupNameUDF = functions.udf(lookupName)\n",
    "\n",
    "# Add a movieTitle column using our new udf\n",
    "moviesWithNames = movieCounts.withColumn(\"movieTitle\", lookupNameUDF(functions.col(\"movieID\")))\n",
    "\n",
    "# Sort the results\n",
    "sortedMoviesWithNames = moviesWithNames.orderBy(functions.desc(\"count\"))\n",
    "\n",
    "# Grab the top 10\n",
    "sortedMoviesWithNames.show(10, False)\n",
    "\n",
    "# Stop the session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc8c047-35ab-455c-bfad-62fa4fc7239a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7996c7f3-000a-4cc3-b799-732b5233682d",
   "metadata": {},
   "source": [
    "### Superheroes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c6b36e2-699e-4e65-94f6-217549b3087f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAPTAIN AMERICA is the most popular superhero with 1933 co-appearances.\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([ \\\n",
    "                     StructField(\"id\", IntegerType(), True), \\\n",
    "                     StructField(\"name\", StringType(), True)])\n",
    "\n",
    "names = spark.read.schema(schema).option(\"sep\", \" \").csv(f\"{DATA_ROOT}/Marvel-names.txt\")\n",
    "\n",
    "lines = spark.read.text(f\"{DATA_ROOT}/Marvel-graph.txt\")\n",
    "\n",
    "# Small tweak vs. what's shown in the video: we trim each line of whitespace as that could\n",
    "# throw off the counts.\n",
    "connections = lines.withColumn(\"id\", func.split(func.trim(func.col(\"value\")), \" \")[0]) \\\n",
    "    .withColumn(\"connections\", func.size(func.split(func.trim(func.col(\"value\")), \" \")) - 1) \\\n",
    "    .groupBy(\"id\").agg(func.sum(\"connections\").alias(\"connections\"))\n",
    "    \n",
    "mostPopular = connections.sort(func.col(\"connections\").desc()).first()\n",
    "\n",
    "mostPopularName = names.filter(func.col(\"id\") == mostPopular[0]).select(\"name\").first()\n",
    "\n",
    "print(mostPopularName[0] + \" is the most popular superhero with \" + str(mostPopular[1]) + \" co-appearances.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9de0ee04-bc45-4f4a-a75f-54774226c685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following characters have only 0 connection(s):\n",
      "+--------------------+\n",
      "|                name|\n",
      "+--------------------+\n",
      "|        BERSERKER II|\n",
      "|              BLARE/|\n",
      "|MARVEL BOY II/MARTIN|\n",
      "|MARVEL BOY/MARTIN BU|\n",
      "|      GIURESCU, RADU|\n",
      "|       CLUMSY FOULUP|\n",
      "|              FENRIS|\n",
      "|              RANDAK|\n",
      "|           SHARKSKIN|\n",
      "|     CALLAHAN, DANNY|\n",
      "|         DEATHCHARGE|\n",
      "|                RUNE|\n",
      "|         SEA LEOPARD|\n",
      "|         RED WOLF II|\n",
      "|              ZANTOR|\n",
      "|JOHNSON, LYNDON BAIN|\n",
      "|          LUNATIK II|\n",
      "|                KULL|\n",
      "|GERVASE, LADY ALYSSA|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([ \\\n",
    "                     StructField(\"id\", IntegerType(), True), \\\n",
    "                     StructField(\"name\", StringType(), True)])\n",
    "\n",
    "names = spark.read.schema(schema).option(\"sep\", \" \").csv(f\"{DATA_ROOT}/Marvel-names.txt\")\n",
    "\n",
    "lines = spark.read.text(f\"{DATA_ROOT}/Marvel-graph.txt\")\n",
    "\n",
    "minConnectionCount = connections.agg(func.min(\"connections\")).first()[0]\n",
    "\n",
    "minConnections = connections.filter(func.col(\"connections\") == minConnectionCount)\n",
    "\n",
    "minConnectionsWithNames = minConnections.join(names, \"id\")\n",
    "\n",
    "print(\"The following characters have only \" + str(minConnectionCount) + \" connection(s):\")\n",
    "\n",
    "minConnectionsWithNames.select(\"name\").show()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c2a81b16-a65a-48eb-bd2a-9fb51ce46c79",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=DegreesOfSeparation, master=local) created by __init__ at /tmp/ipykernel_100/2005043122.py:4 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkConf, SparkContext\n\u001b[1;32m      3\u001b[0m conf \u001b[38;5;241m=\u001b[39m SparkConf()\u001b[38;5;241m.\u001b[39msetMaster(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msetAppName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDegreesOfSeparation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# The characters we wish to find the degree of separation between:\u001b[39;00m\n\u001b[1;32m      7\u001b[0m startCharacterID \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5306\u001b[39m \u001b[38;5;66;03m#SpiderMan\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/context.py:201\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    199\u001b[0m     )\n\u001b[0;32m--> 201\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[1;32m    204\u001b[0m         master,\n\u001b[1;32m    205\u001b[0m         appName,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    215\u001b[0m         memory_profiler_cls,\n\u001b[1;32m    216\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/context.py:449\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    446\u001b[0m     callsite \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\u001b[38;5;241m.\u001b[39m_callsite\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;66;03m# Raise error if there is already a running Spark context\u001b[39;00m\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    450\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot run multiple SparkContexts at once; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    451\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexisting SparkContext(app=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, master=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    452\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m created by \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m at \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    453\u001b[0m         \u001b[38;5;241m%\u001b[39m (\n\u001b[1;32m    454\u001b[0m             currentAppName,\n\u001b[1;32m    455\u001b[0m             currentMaster,\n\u001b[1;32m    456\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mfunction,\n\u001b[1;32m    457\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mfile,\n\u001b[1;32m    458\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mlinenum,\n\u001b[1;32m    459\u001b[0m         )\n\u001b[1;32m    460\u001b[0m     )\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;241m=\u001b[39m instance\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=DegreesOfSeparation, master=local) created by __init__ at /tmp/ipykernel_100/2005043122.py:4 "
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"DegreesOfSeparation\")\n",
    "sc = SparkContext(conf = conf)\n",
    "\n",
    "# The characters we wish to find the degree of separation between:\n",
    "startCharacterID = 5306 #SpiderMan\n",
    "targetCharacterID = 14  #ADAM 3,031 (who?)\n",
    "\n",
    "# Our accumulator, used to signal when we find the target character during\n",
    "# our BFS traversal.\n",
    "hitCounter = sc.accumulator(0)\n",
    "\n",
    "def convertToBFS(line):\n",
    "    fields = line.split()\n",
    "    heroID = int(fields[0])\n",
    "    connections = []\n",
    "    for connection in fields[1:]:\n",
    "        connections.append(int(connection))\n",
    "\n",
    "    color = 'WHITE'\n",
    "    distance = 9999\n",
    "\n",
    "    if (heroID == startCharacterID):\n",
    "        color = 'GRAY'\n",
    "        distance = 0\n",
    "\n",
    "    return (heroID, (connections, distance, color))\n",
    "\n",
    "\n",
    "def createStartingRdd():\n",
    "    inputFile = sc.textFile(f\"{DATA_ROOT}/marvel-graph.txt\")\n",
    "    return inputFile.map(convertToBFS)\n",
    "\n",
    "def bfsMap(node):\n",
    "    characterID = node[0]\n",
    "    data = node[1]\n",
    "    connections = data[0]\n",
    "    distance = data[1]\n",
    "    color = data[2]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    #If this node needs to be expanded...\n",
    "    if (color == 'GRAY'):\n",
    "        for connection in connections:\n",
    "            newCharacterID = connection\n",
    "            newDistance = distance + 1\n",
    "            newColor = 'GRAY'\n",
    "            if (targetCharacterID == connection):\n",
    "                hitCounter.add(1)\n",
    "\n",
    "            newEntry = (newCharacterID, ([], newDistance, newColor))\n",
    "            results.append(newEntry)\n",
    "\n",
    "        #We've processed this node, so color it black\n",
    "        color = 'BLACK'\n",
    "\n",
    "    #Emit the input node so we don't lose it.\n",
    "    results.append( (characterID, (connections, distance, color)) )\n",
    "    return results\n",
    "\n",
    "def bfsReduce(data1, data2):\n",
    "    edges1 = data1[0]\n",
    "    edges2 = data2[0]\n",
    "    distance1 = data1[1]\n",
    "    distance2 = data2[1]\n",
    "    color1 = data1[2]\n",
    "    color2 = data2[2]\n",
    "\n",
    "    distance = 9999\n",
    "    color = 'WHITE'\n",
    "    edges = []\n",
    "\n",
    "    # See if one is the original node with its connections.\n",
    "    # If so preserve them.\n",
    "    if (len(edges1) > 0):\n",
    "        edges.extend(edges1)\n",
    "    if (len(edges2) > 0):\n",
    "        edges.extend(edges2)\n",
    "\n",
    "    # Preserve minimum distance\n",
    "    if (distance1 < distance):\n",
    "        distance = distance1\n",
    "\n",
    "    if (distance2 < distance):\n",
    "        distance = distance2\n",
    "\n",
    "    # Preserve darkest color\n",
    "    if (color1 == 'WHITE' and (color2 == 'GRAY' or color2 == 'BLACK')):\n",
    "        color = color2\n",
    "\n",
    "    if (color1 == 'GRAY' and color2 == 'BLACK'):\n",
    "        color = color2\n",
    "\n",
    "    if (color2 == 'WHITE' and (color1 == 'GRAY' or color1 == 'BLACK')):\n",
    "        color = color1\n",
    "\n",
    "    if (color2 == 'GRAY' and color1 == 'BLACK'):\n",
    "        color = color1\n",
    "\n",
    "    return (edges, distance, color)\n",
    "\n",
    "\n",
    "#Main program here:\n",
    "iterationRdd = createStartingRdd()\n",
    "\n",
    "for iteration in range(0, 10):\n",
    "    print(\"Running BFS iteration# \" + str(iteration+1))\n",
    "\n",
    "    # Create new vertices as needed to darken or reduce distances in the\n",
    "    # reduce stage. If we encounter the node we're looking for as a GRAY\n",
    "    # node, increment our accumulator to signal that we're done.\n",
    "    mapped = iterationRdd.flatMap(bfsMap)\n",
    "\n",
    "    # Note that mapped.count() action here forces the RDD to be evaluated, and\n",
    "    # that's the only reason our accumulator is actually updated.\n",
    "    print(\"Processing \" + str(mapped.count()) + \" values.\")\n",
    "\n",
    "    if (hitCounter.value > 0):\n",
    "        print(\"Hit the target character! From \" + str(hitCounter.value) \\\n",
    "            + \" different direction(s).\")\n",
    "        break\n",
    "\n",
    "    # Reducer combines data for each character ID, preserving the darkest\n",
    "    # color and shortest path.\n",
    "    iterationRdd = mapped.reduceByKey(bfsReduce)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aaf568e-d8f9-43c5-b606-b30b240be6b5",
   "metadata": {},
   "source": [
    "### Min temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea95897-e933-433e-8ca8-6810429d9667",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02e68be5-906b-4a37-b804-79c1aa147b1e",
   "metadata": {},
   "source": [
    "### Customer Orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05662ff-16f8-4883-b630-5bdfd1b76a41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c31af9a-59ff-4c51-b2e6-04635b2f54e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
