{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ef15155-cbf7-4166-9ef0-e011afc8b990",
   "metadata": {},
   "source": [
    "# Section 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e42ebc6-12fa-4ec7-913a-68c98b67b1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as func\n",
    "import collections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ea0a3a-e6cd-498c-88f5-9cc2ce5739bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark session & context\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"Section2\")\n",
    "sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26984ca6-5937-4377-9013-ab3a46689a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = \"../spark_course_resources\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8296144-0521-46e0-a636-11525dbf7776",
   "metadata": {},
   "source": [
    "## Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db51adaf-5e7b-42e7-8b55-10bbbb3ec2c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITE00100554\t5.36F\n",
      "EZE00100082\t7.70F\n"
     ]
    }
   ],
   "source": [
    "# min-temperatures.py\n",
    "def parseLine(line):\n",
    "    fields = line.split(',')\n",
    "    stationID = fields[0]\n",
    "    entryType = fields[2]\n",
    "    temperature = float(fields[3]) * 0.1 * (9.0 / 5.0) + 32.0\n",
    "    return (stationID, entryType, temperature)\n",
    "\n",
    "lines = sc.textFile(f\"{DATA_ROOT}/1800.csv\")\n",
    "parsedLines = lines.map(parseLine)\n",
    "minTemps = parsedLines.filter(lambda x: \"TMIN\" in x[1])\n",
    "stationTemps = minTemps.map(lambda x: (x[0], x[2]))\n",
    "minTemps = stationTemps.reduceByKey(lambda x, y: min(x,y))\n",
    "results = minTemps.collect();\n",
    "\n",
    "for result in results:\n",
    "    print(result[0] + \"\\t{:.2f}F\".format(result[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a829a620-9a4e-4dab-806a-e65ce8d6dfca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITE00100554\t90.14F\n",
      "EZE00100082\t90.14F\n"
     ]
    }
   ],
   "source": [
    "# max-temperatures.py\n",
    "lines = sc.textFile(f\"{DATA_ROOT}/1800.csv\")\n",
    "parsedLines = lines.map(parseLine)\n",
    "minTemps = parsedLines.filter(lambda x: \"TMAX\" in x[1])\n",
    "stationTemps = minTemps.map(lambda x: (x[0], x[2]))\n",
    "minTemps = stationTemps.reduceByKey(lambda x, y: max(x,y))\n",
    "results = minTemps.collect();\n",
    "\n",
    "for result in results:\n",
    "    print(result[0] + \"\\t{:.2f}F\".format(result[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7996c7f3-000a-4cc3-b799-732b5233682d",
   "metadata": {},
   "source": [
    "## Word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c6b36e2-699e-4e65-94f6-217549b3087f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = sc.textFile(f\"{DATA_ROOT}/book.txt\")\n",
    "words = input.flatMap(lambda x: x.split())\n",
    "wordCounts = words.countByValue()\n",
    "\n",
    "for word, count in wordCounts.items():\n",
    "    cleanWord = word.encode('ascii', 'ignore')\n",
    "    if (cleanWord):\n",
    "        pass # print(cleanWord.decode() + \" \" + str(count))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c05662ff-16f8-4883-b630-5bdfd1b76a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def normalizeWords(text):\n",
    "    return re.compile(r'\\W+', re.UNICODE).split(text.lower())\n",
    "\n",
    "words = input.flatMap(normalizeWords)\n",
    "wordCounts = words.countByValue()\n",
    "\n",
    "for word, count in wordCounts.items():\n",
    "    cleanWord = word.encode('ascii', 'ignore')\n",
    "    if (cleanWord):\n",
    "        pass  # print(cleanWord.decode() + \" \" + str(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88a8b050-b296-434a-9bbf-afadf87b6f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort Results\n",
    "wordCounts = words.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y)\n",
    "wordCountsSorted = wordCounts.map(lambda x: (x[1], x[0])).sortByKey()\n",
    "results = wordCountsSorted.collect()\n",
    "\n",
    "for result in results:\n",
    "    count = str(result[0])\n",
    "    word = result[1].encode('ascii', 'ignore')\n",
    "    if (word):\n",
    "        pass  # print(word.decode() + \":\\t\\t\" + count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "22beaf01-e83a-4f9d-af3d-b1f5a459c04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"WordCount\").getOrCreate()\n",
    "\n",
    "# Read each line of my book into a dataframe\n",
    "inputDF = spark.read.text(f\"{DATA_ROOT}/book.txt\")\n",
    "\n",
    "# Split using a regular expression that extracts words\n",
    "words = inputDF.select(func.explode(func.split(inputDF.value, \"\\\\W+\")).alias(\"word\"))\n",
    "wordsWithoutEmptyString = words.filter(words.word != \"\")\n",
    "\n",
    "# Normalize everything to lowercase\n",
    "lowercaseWords = wordsWithoutEmptyString.select(func.lower(wordsWithoutEmptyString.word).alias(\"word\"))\n",
    "\n",
    "# Count up the occurrences of each word\n",
    "wordCounts = lowercaseWords.groupBy(\"word\").count()\n",
    "\n",
    "# Sort by counts\n",
    "wordCountsSorted = wordCounts.sort(\"count\")\n",
    "\n",
    "# Show the results.\n",
    "# wordCountsSorted.show(wordCountsSorted.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f06309e-61aa-468f-879a-2daa1285be4e",
   "metadata": {},
   "source": [
    "## Customer Orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2a83e011-9c79-4e21-aefa-5a84b5032d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44, 4756.8899999999985)\n",
      "(35, 5155.419999999999)\n",
      "(2, 5994.59)\n",
      "(47, 4316.299999999999)\n",
      "(29, 5032.529999999999)\n",
      "(91, 4642.259999999999)\n",
      "(70, 5368.249999999999)\n",
      "(85, 5503.43)\n",
      "(53, 4945.299999999999)\n",
      "(14, 4735.030000000001)\n",
      "(51, 4975.22)\n",
      "(42, 5696.840000000003)\n",
      "(79, 3790.570000000001)\n",
      "(50, 4517.27)\n",
      "(20, 4836.859999999999)\n",
      "(15, 5413.510000000001)\n",
      "(5, 4561.069999999999)\n",
      "(48, 4384.33)\n",
      "(31, 4765.05)\n",
      "(4, 4815.050000000002)\n",
      "(36, 4278.049999999997)\n",
      "(57, 4628.4)\n",
      "(12, 4664.589999999998)\n",
      "(22, 5019.449999999999)\n",
      "(54, 6065.389999999999)\n",
      "(0, 5524.949999999998)\n",
      "(88, 4830.549999999999)\n",
      "(86, 4908.81)\n",
      "(13, 4367.62)\n",
      "(40, 5186.429999999999)\n",
      "(98, 4297.260000000001)\n",
      "(55, 5298.090000000002)\n",
      "(95, 4876.840000000002)\n",
      "(61, 5497.479999999998)\n",
      "(27, 4915.889999999999)\n",
      "(78, 4524.509999999999)\n",
      "(83, 4635.799999999997)\n",
      "(6, 5397.879999999998)\n",
      "(26, 5250.4)\n",
      "(75, 4178.500000000001)\n",
      "(25, 5057.610000000001)\n",
      "(71, 5995.660000000003)\n",
      "(39, 6193.109999999999)\n",
      "(60, 5040.709999999999)\n",
      "(97, 5977.189999999995)\n",
      "(7, 4755.070000000001)\n",
      "(21, 4707.41)\n",
      "(69, 5123.010000000001)\n",
      "(37, 4735.200000000002)\n",
      "(1, 4958.600000000001)\n",
      "(64, 5288.689999999996)\n",
      "(82, 4812.489999999998)\n",
      "(72, 5337.44)\n",
      "(99, 4172.289999999998)\n",
      "(34, 5330.8)\n",
      "(73, 6206.199999999999)\n",
      "(49, 4394.599999999999)\n",
      "(8, 5517.240000000001)\n",
      "(46, 5963.109999999999)\n",
      "(23, 4042.6499999999987)\n",
      "(19, 5059.4299999999985)\n",
      "(65, 5140.3499999999985)\n",
      "(80, 4727.860000000001)\n",
      "(16, 4979.06)\n",
      "(9, 5322.649999999999)\n",
      "(18, 4921.27)\n",
      "(59, 5642.89)\n",
      "(74, 4647.129999999999)\n",
      "(30, 4990.72)\n",
      "(56, 4701.019999999999)\n",
      "(90, 5290.409999999998)\n",
      "(68, 6375.449999999997)\n",
      "(11, 5152.290000000002)\n",
      "(10, 4819.700000000001)\n",
      "(41, 5637.62)\n",
      "(58, 5437.7300000000005)\n",
      "(87, 5206.4)\n",
      "(17, 5032.679999999999)\n",
      "(33, 5254.659999999998)\n",
      "(62, 5253.3200000000015)\n",
      "(92, 5379.280000000002)\n",
      "(76, 4904.209999999999)\n",
      "(66, 4681.919999999999)\n",
      "(43, 5368.83)\n",
      "(52, 5245.059999999999)\n",
      "(77, 4327.729999999999)\n",
      "(81, 5112.709999999999)\n",
      "(84, 4652.939999999999)\n",
      "(3, 4659.63)\n",
      "(93, 5265.750000000001)\n",
      "(89, 4851.479999999999)\n",
      "(45, 3309.38)\n",
      "(24, 5259.920000000003)\n",
      "(96, 3924.230000000001)\n",
      "(67, 4505.79)\n",
      "(63, 5415.150000000001)\n",
      "(94, 4475.569999999999)\n",
      "(32, 5496.050000000004)\n",
      "(38, 4898.460000000002)\n",
      "(28, 5000.709999999998)\n"
     ]
    }
   ],
   "source": [
    "def extract_customer_price_pairs(line):\n",
    "    fields = line.split(',')\n",
    "    return (int(fields[0]), float(fields[2]))\n",
    "\n",
    "lines = sc.textFile(f\"{DATA_ROOT}/customer-orders.csv\")\n",
    "mappedInput = lines.map(extract_customer_price_pairs)\n",
    "total_by_costumer = mappedInput.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "flipped = total_by_costumer.map(lambda x: (x[1], x[0]))\n",
    "\n",
    "totalByCustomerSorted = flipped.sortByKey()\n",
    "\n",
    "results = total_by_costumer.collect();\n",
    "\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c7decfd7-ccc0-4dfe-a9c3-ea5d7bc7a0e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o528.partitions.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/sparkcourse/customer-orders.csv\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:208)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:291)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:291)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:287)\n\tat org.apache.spark.api.java.JavaRDDLike.partitions(JavaRDDLike.scala:61)\n\tat org.apache.spark.api.java.JavaRDDLike.partitions$(JavaRDDLike.scala:61)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\n\tat jdk.internal.reflect.GeneratedMethodAccessor61.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: java.io.IOException: Input path does not exist: file:/sparkcourse/customer-orders.csv\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t... 24 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m sc\u001b[38;5;241m.\u001b[39mtextFile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile:///sparkcourse/customer-orders.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m mappedInput \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mmap(extractCustomerPricePairs)\n\u001b[0;32m----> 7\u001b[0m totalByCustomer \u001b[38;5;241m=\u001b[39m \u001b[43mmappedInput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduceByKey\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m results \u001b[38;5;241m=\u001b[39m totalByCustomer\u001b[38;5;241m.\u001b[39mcollect();\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results:\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/rdd.py:3552\u001b[0m, in \u001b[0;36mRDD.reduceByKey\u001b[0;34m(self, func, numPartitions, partitionFunc)\u001b[0m\n\u001b[1;32m   3505\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreduceByKey\u001b[39m(\n\u001b[1;32m   3506\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRDD[Tuple[K, V]]\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3507\u001b[0m     func: Callable[[V, V], V],\n\u001b[1;32m   3508\u001b[0m     numPartitions: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   3509\u001b[0m     partitionFunc: Callable[[K], \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m portable_hash,\n\u001b[1;32m   3510\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRDD[Tuple[K, V]]\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   3511\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3512\u001b[0m \u001b[38;5;124;03m    Merge the values for each key using an associative and commutative reduce function.\u001b[39;00m\n\u001b[1;32m   3513\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3550\u001b[0m \u001b[38;5;124;03m    [('a', 2), ('b', 1)]\u001b[39;00m\n\u001b[1;32m   3551\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3552\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcombineByKey\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumPartitions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartitionFunc\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/rdd.py:3975\u001b[0m, in \u001b[0;36mRDD.combineByKey\u001b[0;34m(self, createCombiner, mergeValue, mergeCombiners, numPartitions, partitionFunc)\u001b[0m\n\u001b[1;32m   3913\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3914\u001b[0m \u001b[38;5;124;03mGeneric function to combine the elements for each key using a custom\u001b[39;00m\n\u001b[1;32m   3915\u001b[0m \u001b[38;5;124;03mset of aggregation functions.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3972\u001b[0m \u001b[38;5;124;03m[('a', [1, 2]), ('b', [1])]\u001b[39;00m\n\u001b[1;32m   3973\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3974\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m numPartitions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 3975\u001b[0m     numPartitions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_defaultReducePartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3977\u001b[0m serializer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39mserializer\n\u001b[1;32m   3978\u001b[0m memory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_memory_limit()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/rdd.py:4867\u001b[0m, in \u001b[0;36mRDD._defaultReducePartitions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   4865\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39mdefaultParallelism\n\u001b[1;32m   4866\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4867\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetNumPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/rdd.py:5453\u001b[0m, in \u001b[0;36mPipelinedRDD.getNumPartitions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   5452\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetNumPartitions\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m-> 5453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prev_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msize()\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o528.partitions.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/sparkcourse/customer-orders.csv\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:208)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:291)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:291)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:287)\n\tat org.apache.spark.api.java.JavaRDDLike.partitions(JavaRDDLike.scala:61)\n\tat org.apache.spark.api.java.JavaRDDLike.partitions$(JavaRDDLike.scala:61)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\n\tat jdk.internal.reflect.GeneratedMethodAccessor61.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: java.io.IOException: Input path does not exist: file:/sparkcourse/customer-orders.csv\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t... 24 more\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5e213e-ae43-49ad-9b5f-767d5eaea5a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
